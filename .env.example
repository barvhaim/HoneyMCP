# **** LLM (Large Language Model) configuration ****
# LLM Provider Options: ollama, watsonx, openai
LLM_PROVIDER=watsonx
# LLM_MODEL=meta-llama/llama-3-3-70b-instruct
LLM_MODEL=openai/gpt-oss-120b

# * watsonx.ai configuration if LLM_PROVIDER is set to 'watsonx'
WATSONX_URL=
WATSONX_APIKEY=
WATSONX_PROJECT_ID=

TEMPERATURE=0

# * Observability 
OBSERVABILITY_PLATFORM=None

# * Langsmith configuration
LANGSMITH_TRACING=false
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_API_KEY=
LANGSMITH_PROJECT=

